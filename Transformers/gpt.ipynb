{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from dataclasses import dataclass\n",
    "from torch import nn\n",
    "\n",
    "@dataclass\n",
    "class config: \n",
    "    vocab_size: int \n",
    "    embedding_dim: int = 768\n",
    "    num_attention_heads: int = 12\n",
    "    num_attention_blocks: int = 12\n",
    "    ff_hidden_dim: int = 4*768\n",
    "    bias: bool = True\n",
    "    \n",
    "\n",
    "class causal_attention_head(nn.Module):\n",
    "    def __init__(self, config: config):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding_dim = config.embedding_dim\n",
    "        self.head_size = self.embedding_dim // config.num_attention_heads\n",
    "        \n",
    "        # There are four matrices W_q, W_k, W_v, W_o\n",
    "        # head_size, embedding_dim\n",
    "        self.W_q = nn.Parameter(torch.zeros(self.head_size, self.embedding_dim))\n",
    "        self.W_k = nn.Parameter(torch.zeros(self.head_size, self.embedding_dim))\n",
    "        self.W_v = nn.Parameter(torch.zeros(self.head_size, self.embedding_dim))\n",
    "        \n",
    "        torch.nn.init.normal_(self.W_q, mean=0.0, std=0.02)\n",
    "        torch.nn.init.normal_(self.W_k, mean=0.0, std=0.02)\n",
    "        torch.nn.init.normal_(self.W_v, mean=0.0, std=0.02)\n",
    "            \n",
    "    def forward(self, X, padding_mask):\n",
    "        #X: batch, seq, features \n",
    "        #padding: batch, seq\n",
    "        \n",
    "        #we needs to make it (batch, seq, 1) <- this allows procasting along dim=2\n",
    "        padding_max = padding_mask.unsqueeze(2)\n",
    "        X = X * padding_max\n",
    "        \n",
    "        seq_len = X.shape[1]\n",
    "        \n",
    "        #: batch, seq, head_size\n",
    "        X_q = X @ self.W_q.T\n",
    "        X_k = X @ self.W_k.T\n",
    "        X_v = X @ self.W_v.T\n",
    "        \n",
    "        causal_attention_mask = torch.tril(torch.ones(seq_len, seq_len)).unsqueeze(0)\n",
    "        \n",
    "        \n",
    "        #Each element in the row i represents how much of key k_j (j in head_size) is similar??? to query v_i\n",
    "        scaled_attention_scores = torch.bmm(X_q, X_k.transpose(2,1)) / (self.head_size ** 0.5) # batch, seq, seq\n",
    "        attention = torch.softmax(scaled_attention_scores.masked_fill(causal_attention_mask==0, float('-inf')), dim=2) # batch, seq, seq\n",
    "        attention = torch.bmm(attention, X_v) # batch, seq, head_size\n",
    "        \n",
    "        return attention\n",
    "\n",
    "class self_attention(nn.Module):\n",
    "    def __init__(self, config: config):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding_dim = config.embedding_dim\n",
    "        self.num_heads = config.num_attention_heads\n",
    "        self.head_size = self.embedding_dim // config.num_attention_heads\n",
    "        \n",
    "  \n",
    "        self.attention_heads = nn.ModuleList([\n",
    "            causal_attention_head(config) \n",
    "            for _ in range(self.num_heads)\n",
    "        ])\n",
    "        \n",
    "        self.W_o = nn.Linear(self.embedding_dim, self.embedding_dim) \n",
    "        \n",
    "        \n",
    "    def forward(self, X, padding_mask):\n",
    "        #Each element: batch, seq, head_size\n",
    "        head_outputs = []\n",
    "        for head in self.attention_heads:\n",
    "            head_outputs.append(head(X, padding_mask))\n",
    "        \n",
    "        # Concatenate all head outputs\n",
    "        #batch, seq, embedding_dim\n",
    "        concatenated = torch.cat(head_outputs, dim=-1)\n",
    "        \n",
    "        # Apply output projection\n",
    "        output = self.W_o(concatenated)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "\n",
    "class transformer_block(nn.Module):\n",
    "    def __init__(self, config:config):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.attention_block = self_attention(config)\n",
    "        self.layerNorm = nn.LayerNorm(config.embedding_dim, bias=config.bias)\n",
    "        \n",
    "        self.ff_hidden_dim = config.ff_hidden_dim\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(config.embedding_dim, self.ff_hidden_dim, bias=config.bias), #bias = True\n",
    "            nn.GELU(),\n",
    "            nn.Linear(self.ff_hidden_dim, config.embedding_dim, bias=config.bias), #bias = True\n",
    "            nn.GELU()\n",
    "        )\n",
    "        \n",
    "    def forward(self, X, padding_mask):\n",
    "        #X: batch, seq, features \n",
    "        \n",
    "        self_attention_out = self.layerNorm(X + self.attention_block(X, padding_mask))\n",
    "        linear_out = self.layerNorm(self_attention_out + self.linear(self_attention_out))\n",
    "        \n",
    "        return linear_out\n",
    "\n",
    "\n",
    "\n",
    "class GPT1(nn.Module):\n",
    "        def __init__(self, config: config):\n",
    "            super().__init__() \n",
    "            \n",
    "            self.token_embedding = nn.Embedding(config.vocab_size, config.embedding_dim)\n",
    "            \n",
    "            self.drop = nn.Dropout(config.dropout)\n",
    "              \n",
    "            #batch, seq, embedding_dim\n",
    "            self.transformer = nn.ModuleList([transformer_block(config) for _ in range(config.num_attention_blocks)])\n",
    "        \n",
    "            #embedding_dim, vocab_size -> batch, seq, vocab_size\n",
    "            self.lm_head = nn.Linear(config.embedding_dim, config.vocab_size, bias=False)\n",
    "            \n",
    "            self.apply(self._init_weights)\n",
    "        \n",
    "        \n",
    "        def forward(self, X, padding_mask):\n",
    "            # X: batch, seq\n",
    "            \n",
    "            X = self.token_embedding(X)\n",
    "            \n",
    "            X = self.drop(X)\n",
    "            \n",
    "            for block in self.transformer:\n",
    "                X = block(X, padding_mask)\n",
    "            \n",
    "            out = self.lm_head(X)\n",
    "            \n",
    "            return out\n",
    "        \n",
    "        def _init_weights(self, module):\n",
    "            if isinstance(module, nn.Embedding):\n",
    "                torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if isinstance(module, nn.Linear):\n",
    "                torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "                if module.bias is not None:\n",
    "                    torch.nn.init.zeros_(module.bias)\n",
    "            elif isinstance(module, nn.LayerNorm):\n",
    "                torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "                if module.bias is not None:\n",
    "                    torch.nn.init.zeros_(module.bias)\n",
    "                \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs: [15496, 995, 11, 314, 1101, 4856, 402, 11571, 12, 17, 347, 11401, 0]\n",
      "Decoded text: Hello world, I'm testing GPT-2 BPE!\n",
      "Tokens: ['Hello', ' world', ',', ' I', \"'m\", ' testing', ' G', 'PT', '-', '2', ' B', 'PE', '!']\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "# Load the GPT-2 tokenizer\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "# Example sentence\n",
    "sentence = \"Hello world, I'm testing GPT-2 BPE!\"\n",
    "\n",
    "# Tokenize into token IDs\n",
    "token_ids = enc.encode(sentence)\n",
    "print(\"Token IDs:\", token_ids)\n",
    "\n",
    "# Decode back to string\n",
    "decoded = enc.decode(token_ids)\n",
    "print(\"Decoded text:\", decoded)\n",
    "\n",
    "# If you want tokens as strings\n",
    "tokens = [enc.decode([tid]) for tid in token_ids]\n",
    "print(\"Tokens:\", tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 3, 5])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X = torch.ones(1,3,5)\n",
    "X = torch.stack([X, torch.ones_like(X), torch.ones_like(X)], dim=1)\n",
    "attention_mask = torch.tril(torch.ones_like(X))\n",
    "print(attention_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padding_max = torch.ones(3,3).masked_fill(torch.tril(torch.ones(3,3)) == 0, float('-inf'))\n",
    "torch.softmax(padding_max, dim=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "playground",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
